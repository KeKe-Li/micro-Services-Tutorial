### micro-Services-Tutorial
 微服务最早由Martin Fowler与James Lewis于2014年共同提出，微服务架构风格是一种使用一套小服务来开发单个应用的方式途径，每个服务运行在自己的进程中，并使用轻量级机制通信，通常是HTTP API，这些服务基于业务能力构建，并能够通过自动化部署机制来独立部署，这些服务使用不同的编程语言实现，以及不同数据存储技术，并保持最低限度的集中式管理。然而微服务又需要限流器(Rate Limiter)，数据传输(Trasport 序列化和反序列化),日志(Logging),指标(Metrics)
,断路器(Circuit breaker),请求追踪(Request tracing ),服务发现(Service Discovery),因此就想写一篇关于微服务和微服务组件的总结来记录下自己使用优化的过程．

#### Ceph分布式文件共享方案

在使用Kubeneters的时候在各个容器中都需要使用同一套文件,但是如果使用NAS盘的方式话，无论是在更新或者是读取时候都要几分钟时间去部署，而且有时候还会出现文件占用失败的问题,调研了一段时间之后发现Kuberneters结合比较好的文件系统用Ceph分布式文件共享方案去解决这样的问题．


* 环境准备
在安装分布式之前需要先进行环境准备,需要3台服务器来做集群,ceph默认会进行保存三份文件来保障数据安全,服务器系统是centos7.3:

| 主机名| IP   | 部署服务  |
| ---- | ---------- |--------|
|host1| 120.92.172.35|主机1|
|host2| 120.92.169.191|主机2|
|host3| 120.92.165.229 |主机3|

首先安装docker环境，这个可以根据电脑系统的不同，选择不同的安装方式。

* [Mac安装](https://docs.docker.com/docker-for-mac/install/)
* [Ubantu安装](https://docs.docker.com/install/linux/docker-ce/ubuntu/)
* [Windows安装](https://docs.docker.com/docker-for-windows/install/)
* [centos安装](https://docs.docker.com/install/linux/docker-ce/centos/)

我这里是用脚本直接在centos上直接安装的:

```bash
yum-config-manager \
    --add-repo \
    https://download.docker.com/linux/centos/docker-ce.repo;

yum-config-manager --enable docker-ce-edge;

yum-config-manager --disable docker-ce-edge;

yum install docker-ce;

systemctl start docker.service;
systemctl enable docker.service;
```
安装成功之后可以查看下:

```bash
> docker --verison
Docker version 18.06.0-ce, build 0ffa825
```
注意:这里需要配置好国内docker源，这样可以提高速度
```bash
> mkdir -p /etc/docker
> tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://xxxxxxxx.mirror.aliyuncs.com"]
}
EOF

> systemctl daemon-reload
> systemctl restart docker
> systemctl enable docker
```
#### 部署Ceph集群

在主机1上部署运行mon，用以让客户端连接 Ceph 监视器：
```bash
node1> docker run -d \
        --name=mon \
        --net=host \
        -v /etc/ceph:/etc/ceph \
        -v /var/lib/ceph/:/var/lib/ceph/ \
        -e MON_IP=120.92.172.35 \
        -e CEPH_PUBLIC_NETWORK=192.168.1.0/24 \
        ceph/daemon mon
```
查看docker和ceph运行状态:
```bash
> docker ps
CONTAINER ID        IMAGE               COMMAND                CREATED              STATUS              PORTS               NAMES
b0c9d7680461        ceph/daemon         "/entrypoint.sh mon"   About a minute ago   Up About a 

> docker exec mon ceph -s
cluster:
    id:     da8f7f5b-b767-4420-a510-287f4ced25de
    health: HEALTH_OK
 
  services:
    mon: 1 daemons, quorum ceph-1
    mgr: no daemons active
    osd: 0 osds: 0 up, 0 in
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0  objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:     
```
由于我们没有映射端口,但是Ceph容器会把后续需要使用到的端口映射出来:
```bash
node1> yum install tcping
node1> tcping 120.92.172.35 6789
120.92.172.35 port 6789 open
```
通常Ceph有两种方式实现在各个节点之间共享配置,第一种是以文件的方式实现共享，需要把第一个启动起来的节点的文件CP到其他节点,另外一种是使用配置服务(比如etcd或者consul). 这里我使用的第一种共享配置，将主机１上的配置文件复制到主机2和主机3,复制的路径包含/etc/ceph和/var/lib/ceph/bootstrap-*下的所有内容。

```bash
node2 > mkdir -p /var/lib/ceph
node1> scp -r /etc/ceph root@120.92.169.191:/etc
node1> scp -r /var/lib/ceph/bootstrap* root@120.92.169.191:/var/lib/ceph

node3 > mkdir -p /var/lib/ceph
node1 > scp -r /etc/ceph root@192.168.3.103:/etc
node1 > scp -r /var/lib/ceph/bootstrap* root@120.92.165.229:/var/lib/ceph
```
启动主机2和主机3上的mon,在主机2上执行以下命令启动 mon,修改 MON_IP:
```bash
> docker run -d \
        --net=host \
        --name=mon \
        -v /etc/ceph:/etc/ceph \
        -v /var/lib/ceph/:/var/lib/ceph/ \
        -e MON_IP=120.92.169.191 \
        -e CEPH_PUBLIC_NETWORK=192.168.1.0/24 \
        ceph/daemon mon
```
在主机３上执行以下命令启动 mon,修改 MON_IP:
```bash
> docker run -d \
        --net=host \
        --name=mon \
        -v /etc/ceph:/etc/ceph \
        -v /var/lib/ceph/:/var/lib/ceph/ \
        -e MON_IP=120.92.165.229 \
        -e CEPH_PUBLIC_NETWORK=192.168.1.0/24 \
        ceph/daemon mon
```
在节点上在查看ceph集群状态可以看到有三个mos上线了:
```bash
> docker exec mon ceph -s
  cluster:
    id:     da8f7f5b-b767-4420-a510-287f4ced25de
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3
    mgr: no daemons active
    osd: 0 osds: 0 up, 0 in
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0  objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:  
 ```
然后需要运行osd服务把磁盘给ceph作为文件共享系统的基础存储,因此我们需要为三台服务器挂载对应的磁盘(这里选择挂载100GB的SSD云磁盘)．
```bash
> fdisk -l
Disk /dev/vda: 42.9 GB, 42949672960 bytes, 83886080 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x0008d73a

   Device Boot      Start         End      Blocks   Id  System
/dev/vda1   *        2048    83884031    41940992   53  Linux

Disk /dev/vdb: 107.5 GB, 107374182400 bytes, 209715200 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
```
这时候在三台服务器上分别运行(如果需要挂载多块可以运行多个osd):
```docker
> docker run -d \
        --net=host \
        --name=osd \
        -v /etc/ceph:/etc/ceph \
        -v /var/lib/ceph/:/var/lib/ceph/ \
        -v /dev/:/dev/ \
        --privileged=true \
        -e OSD_FORCE_ZAP=1 \
        -e OSD_DEVICE=/dev/vdb \
        ceph/daemon osd_ceph_disk
```
 此时我们查看发现osd3个节点就可以看到已经上线了:
 ```bash
 >  docker exec mon ceph -s
  cluster:
    id:     da8f7f5b-b767-4420-a510-287f4ced25de
    health: HEALTH_WARN
            no active mgr
 
  services:
    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3
    mgr: no daemons active
    osd: 3 osds: 3 up, 3 in
 
  data:
    pools:   0 pools, 0 pgs
    objects: 0  objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:     

 ```
但是这个时候我们还看不到存储空间,这是因为mgr没有启动,我们把mds和rgw也一起跑起来在node1节点上:
```bash
> docker run -d \
        --net=host \
        --name=mgr \
        -v /etc/ceph:/etc/ceph \
        -v /var/lib/ceph/:/var/lib/ceph/ \
        ceph/daemon mgr
        
> docker run -d \
        --net=host \
        --name=mds \
        -v /etc/ceph:/etc/ceph \
        -v /var/lib/ceph/:/var/lib/ceph/ \
        -e CEPHFS_CREATE=1 \
        ceph/daemon mds     
> docker run -d \
        --name=rgw \
        -p 80:80 \
        -v /etc/ceph:/etc/ceph \
        -v /var/lib/ceph/:/var/lib/ceph/ \
        ceph/daemon rgw        
```
这个时候就可以看到初始化完成了:
```bash
>  docker exec mon ceph -s
  cluster:
    id:     da8f7f5b-b767-4420-a510-287f4ced25de
    health: HEALTH_OK
 
  services:
    mon: 3 daemons, quorum ceph-1,ceph-2,ceph-3
    mgr: ceph-1(active)
    mds: cephfs-1/1/1 up  {0=ceph-1=up:active}
    osd: 3 osds: 3 up, 3 in
    rgw: 1 daemon active
 
  data:
    pools:   6 pools, 48 pgs
    objects: 209  objects, 3.4 KiB
    usage:   6.0 GiB used, 292 GiB / 298 GiB avail
    pgs:     48 active+clean
```
#### 使用Ceph
我们可以使用ceph挂载磁盘进行使用,但是ceph有很强的用户校验机制所以这里需要先拿到访问key ,而且在使用过程中可以指定多个节点形成负载均衡，然后用[内核驱动挂载 Ceph 文件系统 — Ceph Documentation](http://docs.ceph.org.cn/cephfs/kernel/).
```bash
> mkdir /mnt/cephfile
> cat /etc/ceph/ceph.client.admin.keyring 
[client.admin]
        key = AQBXv0VbKtikExAAwHHp+F2HJSnYIIqaozjt3g==
        auid = 0
        caps mds = "allow"
        caps mgr = "allow *"
        caps mon = "allow *"
        caps osd = "allow *"

mount -t ceph 120.92.172.35,120.92.169.191,120.92.165.229:/ /mnt/cephfile -o name=admin,secret=AQBhlz1bZBHcLxAAt6eIyBxnAxFoyA7PDTqAkQ==
umount /mnt/cephfile

>  lsof  /mnt/cephfile
COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF          NODE NAME
bash    10356 root  cwd    DIR    0,0        2 1099511627781 /mnt/cephfile/data/dev/code
```
这是最简单的使用,还有关于rdb对象存储以及Ceph还有快照等功能，可以在搭建好整个集群之后进行测试文件速度(这里主要测试的是5000多个小文件拷贝速度)如下:
```bash
# NAS盘
>  time cp -rf php/ general/test
real    2m 7.05s
user    0m 0.13s
sys     0m 1.80s

# CEPH
> time cp -rf php/ generalceph/test
real    0m 6.88s
user    0m 0.05s
sys     0m 0.39s

# 本地磁盘
> time cp -rf php/ php2
real    0m 1.16s
user    0m 0.02s
sys     0m 0.21s
```
通过对比可以看到CEPH的速度对于NAS盘提升有很多,面对本地磁盘差距也不是很大.
